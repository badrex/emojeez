{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import sys\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any, Tuple, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_ranges = [\n",
    "    (0x1F600, 0x1F64F),  # Emoticons\n",
    "    (0x1F300, 0x1F5FF),  # Miscellaneous Symbols and Pictographs\n",
    "    (0x1F680, 0x1F6FF),  # Transport and Map Symbols\n",
    "    (0x1F700, 0x1F77F),  # Alchemical Symbols\n",
    "    (0x2600, 0x26FF),    # Miscellaneous Symbols\n",
    "    (0x2700, 0x27BF),    # Dingbats\n",
    "    (0x2B50, 0x2BFF),    # Additional symbols\n",
    "]\n",
    "\n",
    "def is_emoji(character):\n",
    "    return any(start <= ord(character) <= end for start, end in emoji_ranges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1702"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "START, END = ord(' '), sys.maxunicode + 1\n",
    "\n",
    "emojis = []\n",
    "    \n",
    "for code in range(START, END):\n",
    "    char = chr(code)\n",
    "\n",
    "    name = unicodedata.name(char, None)\n",
    "\n",
    "    #print(f'U+{code:04X}\\t{char}\\t{name}')\n",
    "    if is_emoji(char) and name:\n",
    "        emojis.append(\n",
    "            {\n",
    "                'code': ord(char),\n",
    "                'char': char,\n",
    "                'name': name.capitalize()\n",
    "            }\n",
    "        )\n",
    "\n",
    "len(emojis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'code': 128123, 'char': 'ðŸ‘»', 'name': 'Ghost'}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emojis[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastembed import TextEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>dim</th>\n",
       "      <th>description</th>\n",
       "      <th>size_in_GB</th>\n",
       "      <th>model_file</th>\n",
       "      <th>additional_files</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BAAI/bge-small-en-v1.5</td>\n",
       "      <td>384</td>\n",
       "      <td>Fast and Default English model</td>\n",
       "      <td>0.067</td>\n",
       "      <td>model_optimized.onnx</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BAAI/bge-small-zh-v1.5</td>\n",
       "      <td>512</td>\n",
       "      <td>Fast and recommended Chinese model</td>\n",
       "      <td>0.090</td>\n",
       "      <td>model_optimized.onnx</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sentence-transformers/all-MiniLM-L6-v2</td>\n",
       "      <td>384</td>\n",
       "      <td>Sentence Transformer model, MiniLM-L6-v2</td>\n",
       "      <td>0.090</td>\n",
       "      <td>model.onnx</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>snowflake/snowflake-arctic-embed-xs</td>\n",
       "      <td>384</td>\n",
       "      <td>Based on all-MiniLM-L6-v2 model with only 22m ...</td>\n",
       "      <td>0.090</td>\n",
       "      <td>onnx/model.onnx</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jinaai/jina-embeddings-v2-small-en</td>\n",
       "      <td>512</td>\n",
       "      <td>English embedding model supporting 8192 sequen...</td>\n",
       "      <td>0.120</td>\n",
       "      <td>onnx/model.onnx</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>snowflake/snowflake-arctic-embed-s</td>\n",
       "      <td>384</td>\n",
       "      <td>Based on infloat/e5-small-unsupervised, does n...</td>\n",
       "      <td>0.130</td>\n",
       "      <td>onnx/model.onnx</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>BAAI/bge-small-en</td>\n",
       "      <td>384</td>\n",
       "      <td>Fast English model</td>\n",
       "      <td>0.130</td>\n",
       "      <td>model_optimized.onnx</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>nomic-ai/nomic-embed-text-v1.5-Q</td>\n",
       "      <td>768</td>\n",
       "      <td>Quantized 8192 context length english model</td>\n",
       "      <td>0.130</td>\n",
       "      <td>onnx/model_quantized.onnx</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>BAAI/bge-base-en-v1.5</td>\n",
       "      <td>768</td>\n",
       "      <td>Base English model, v1.5</td>\n",
       "      <td>0.210</td>\n",
       "      <td>model_optimized.onnx</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sentence-transformers/paraphrase-multilingual-...</td>\n",
       "      <td>384</td>\n",
       "      <td>Sentence Transformer model, paraphrase-multili...</td>\n",
       "      <td>0.220</td>\n",
       "      <td>model_optimized.onnx</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Qdrant/clip-ViT-B-32-text</td>\n",
       "      <td>512</td>\n",
       "      <td>CLIP text encoder</td>\n",
       "      <td>0.250</td>\n",
       "      <td>model.onnx</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>BAAI/bge-base-en</td>\n",
       "      <td>768</td>\n",
       "      <td>Base English model</td>\n",
       "      <td>0.420</td>\n",
       "      <td>model_optimized.onnx</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>snowflake/snowflake-arctic-embed-m</td>\n",
       "      <td>768</td>\n",
       "      <td>Based on intfloat/e5-base-unsupervised model, ...</td>\n",
       "      <td>0.430</td>\n",
       "      <td>onnx/model.onnx</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>nomic-ai/nomic-embed-text-v1</td>\n",
       "      <td>768</td>\n",
       "      <td>8192 context length english model</td>\n",
       "      <td>0.520</td>\n",
       "      <td>onnx/model.onnx</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>jinaai/jina-embeddings-v2-base-en</td>\n",
       "      <td>768</td>\n",
       "      <td>English embedding model supporting 8192 sequen...</td>\n",
       "      <td>0.520</td>\n",
       "      <td>onnx/model.onnx</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>nomic-ai/nomic-embed-text-v1.5</td>\n",
       "      <td>768</td>\n",
       "      <td>8192 context length english model</td>\n",
       "      <td>0.520</td>\n",
       "      <td>onnx/model.onnx</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>snowflake/snowflake-arctic-embed-m-long</td>\n",
       "      <td>768</td>\n",
       "      <td>Based on nomic-ai/nomic-embed-text-v1-unsuperv...</td>\n",
       "      <td>0.540</td>\n",
       "      <td>onnx/model.onnx</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>mixedbread-ai/mxbai-embed-large-v1</td>\n",
       "      <td>1024</td>\n",
       "      <td>MixedBread Base sentence embedding model, does...</td>\n",
       "      <td>0.640</td>\n",
       "      <td>onnx/model.onnx</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>sentence-transformers/paraphrase-multilingual-...</td>\n",
       "      <td>768</td>\n",
       "      <td>Sentence-transformers model for tasks like clu...</td>\n",
       "      <td>1.000</td>\n",
       "      <td>onnx/model.onnx</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>snowflake/snowflake-arctic-embed-l</td>\n",
       "      <td>1024</td>\n",
       "      <td>Based on intfloat/e5-large-unsupervised, large...</td>\n",
       "      <td>1.020</td>\n",
       "      <td>onnx/model.onnx</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>BAAI/bge-large-en-v1.5</td>\n",
       "      <td>1024</td>\n",
       "      <td>Large English model, v1.5</td>\n",
       "      <td>1.200</td>\n",
       "      <td>model.onnx</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>thenlper/gte-large</td>\n",
       "      <td>1024</td>\n",
       "      <td>Large general text embeddings model</td>\n",
       "      <td>1.200</td>\n",
       "      <td>model.onnx</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>intfloat/multilingual-e5-large</td>\n",
       "      <td>1024</td>\n",
       "      <td>Multilingual model, e5-large. Recommend using ...</td>\n",
       "      <td>2.240</td>\n",
       "      <td>model.onnx</td>\n",
       "      <td>[model.onnx_data]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                model   dim  \\\n",
       "0                              BAAI/bge-small-en-v1.5   384   \n",
       "1                              BAAI/bge-small-zh-v1.5   512   \n",
       "2              sentence-transformers/all-MiniLM-L6-v2   384   \n",
       "3                 snowflake/snowflake-arctic-embed-xs   384   \n",
       "4                  jinaai/jina-embeddings-v2-small-en   512   \n",
       "5                  snowflake/snowflake-arctic-embed-s   384   \n",
       "6                                   BAAI/bge-small-en   384   \n",
       "7                    nomic-ai/nomic-embed-text-v1.5-Q   768   \n",
       "8                               BAAI/bge-base-en-v1.5   768   \n",
       "9   sentence-transformers/paraphrase-multilingual-...   384   \n",
       "10                          Qdrant/clip-ViT-B-32-text   512   \n",
       "11                                   BAAI/bge-base-en   768   \n",
       "12                 snowflake/snowflake-arctic-embed-m   768   \n",
       "13                       nomic-ai/nomic-embed-text-v1   768   \n",
       "14                  jinaai/jina-embeddings-v2-base-en   768   \n",
       "15                     nomic-ai/nomic-embed-text-v1.5   768   \n",
       "16            snowflake/snowflake-arctic-embed-m-long   768   \n",
       "17                 mixedbread-ai/mxbai-embed-large-v1  1024   \n",
       "18  sentence-transformers/paraphrase-multilingual-...   768   \n",
       "19                 snowflake/snowflake-arctic-embed-l  1024   \n",
       "20                             BAAI/bge-large-en-v1.5  1024   \n",
       "21                                 thenlper/gte-large  1024   \n",
       "22                     intfloat/multilingual-e5-large  1024   \n",
       "\n",
       "                                          description  size_in_GB  \\\n",
       "0                      Fast and Default English model       0.067   \n",
       "1                  Fast and recommended Chinese model       0.090   \n",
       "2            Sentence Transformer model, MiniLM-L6-v2       0.090   \n",
       "3   Based on all-MiniLM-L6-v2 model with only 22m ...       0.090   \n",
       "4   English embedding model supporting 8192 sequen...       0.120   \n",
       "5   Based on infloat/e5-small-unsupervised, does n...       0.130   \n",
       "6                                  Fast English model       0.130   \n",
       "7         Quantized 8192 context length english model       0.130   \n",
       "8                            Base English model, v1.5       0.210   \n",
       "9   Sentence Transformer model, paraphrase-multili...       0.220   \n",
       "10                                  CLIP text encoder       0.250   \n",
       "11                                 Base English model       0.420   \n",
       "12  Based on intfloat/e5-base-unsupervised model, ...       0.430   \n",
       "13                  8192 context length english model       0.520   \n",
       "14  English embedding model supporting 8192 sequen...       0.520   \n",
       "15                  8192 context length english model       0.520   \n",
       "16  Based on nomic-ai/nomic-embed-text-v1-unsuperv...       0.540   \n",
       "17  MixedBread Base sentence embedding model, does...       0.640   \n",
       "18  Sentence-transformers model for tasks like clu...       1.000   \n",
       "19  Based on intfloat/e5-large-unsupervised, large...       1.020   \n",
       "20                          Large English model, v1.5       1.200   \n",
       "21                Large general text embeddings model       1.200   \n",
       "22  Multilingual model, e5-large. Recommend using ...       2.240   \n",
       "\n",
       "                   model_file   additional_files  \n",
       "0        model_optimized.onnx                NaN  \n",
       "1        model_optimized.onnx                NaN  \n",
       "2                  model.onnx                NaN  \n",
       "3             onnx/model.onnx                NaN  \n",
       "4             onnx/model.onnx                NaN  \n",
       "5             onnx/model.onnx                NaN  \n",
       "6        model_optimized.onnx                NaN  \n",
       "7   onnx/model_quantized.onnx                NaN  \n",
       "8        model_optimized.onnx                NaN  \n",
       "9        model_optimized.onnx                NaN  \n",
       "10                 model.onnx                NaN  \n",
       "11       model_optimized.onnx                NaN  \n",
       "12            onnx/model.onnx                NaN  \n",
       "13            onnx/model.onnx                NaN  \n",
       "14            onnx/model.onnx                NaN  \n",
       "15            onnx/model.onnx                NaN  \n",
       "16            onnx/model.onnx                NaN  \n",
       "17            onnx/model.onnx                NaN  \n",
       "18            onnx/model.onnx                NaN  \n",
       "19            onnx/model.onnx                NaN  \n",
       "20                 model.onnx                NaN  \n",
       "21                 model.onnx                NaN  \n",
       "22                 model.onnx  [model.onnx_data]  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "supported_models = (\n",
    "    pd.DataFrame(TextEmbedding.list_supported_models())\n",
    "    .sort_values(\"size_in_GB\")\n",
    "    .drop(columns=\"sources\")\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "supported_models\n",
    "\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "supported_models.loc[9]['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents: List[str] = [\n",
    "#     \"passage: Hello, World!\",\n",
    "#     \"query: Hello, World!\",\n",
    "#     \"passage: This is an example passage.\",\n",
    "#     \"fastembed is supported by and maintained by Qdrant.\"\n",
    "# ]\n",
    "# embedding_model = TextEmbedding('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "# embeddings: List[np.ndarray] = list(embedding_model.embed(documents))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.array(embeddings).shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import models, QdrantClient\n",
    "from sentence_transformers import SentenceTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67608cd0ded64f61a3d868a314769e4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62c5070e43cb49c1a81359038216d263",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "213defb01c9144108d1e3b1e70f93ac1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/4.12k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "494a3aef7e854e388ea8af5b74ed3e3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/badr/anaconda3/envs/emoscope/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8abc3f191e8243f7ae4a7faf5b34635d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/645 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "979a3e2c65e742168f63bf1da2cd79f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/471M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6985565a05d464cb91d680935f5484b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/480 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7de1a26cec364694bd5661fc5b7db946",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e08954f8dfd941098817af512e17a19b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2832d1168084283bf52d6da5c0f694b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoder = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = QdrantClient(\":memory:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    {\n",
    "        \"name\": \"The Time Machine\",\n",
    "        \"description\": \"A man travels through time and witnesses the evolution of humanity.\",\n",
    "        \"author\": \"H.G. Wells\",\n",
    "        \"year\": 1895,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Ender's Game\",\n",
    "        \"description\": \"A young boy is trained to become a military leader in a war against an alien race.\",\n",
    "        \"author\": \"Orson Scott Card\",\n",
    "        \"year\": 1985,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Brave New World\",\n",
    "        \"description\": \"A dystopian society where people are genetically engineered and conditioned to conform to a strict social hierarchy.\",\n",
    "        \"author\": \"Aldous Huxley\",\n",
    "        \"year\": 1932,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"The Hitchhiker's Guide to the Galaxy\",\n",
    "        \"description\": \"A comedic science fiction series following the misadventures of an unwitting human and his alien friend.\",\n",
    "        \"author\": \"Douglas Adams\",\n",
    "        \"year\": 1979,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Dune\",\n",
    "        \"description\": \"A desert planet is the site of political intrigue and power struggles.\",\n",
    "        \"author\": \"Frank Herbert\",\n",
    "        \"year\": 1965,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Foundation\",\n",
    "        \"description\": \"A mathematician develops a science to predict the future of humanity and works to save civilization from collapse.\",\n",
    "        \"author\": \"Isaac Asimov\",\n",
    "        \"year\": 1951,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Snow Crash\",\n",
    "        \"description\": \"A futuristic world where the internet has evolved into a virtual reality metaverse.\",\n",
    "        \"author\": \"Neal Stephenson\",\n",
    "        \"year\": 1992,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Neuromancer\",\n",
    "        \"description\": \"A hacker is hired to pull off a near-impossible hack and gets pulled into a web of intrigue.\",\n",
    "        \"author\": \"William Gibson\",\n",
    "        \"year\": 1984,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"The War of the Worlds\",\n",
    "        \"description\": \"A Martian invasion of Earth throws humanity into chaos.\",\n",
    "        \"author\": \"H.G. Wells\",\n",
    "        \"year\": 1898,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"The Hunger Games\",\n",
    "        \"description\": \"A dystopian society where teenagers are forced to fight to the death in a televised spectacle.\",\n",
    "        \"author\": \"Suzanne Collins\",\n",
    "        \"year\": 2008,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"The Andromeda Strain\",\n",
    "        \"description\": \"A deadly virus from outer space threatens to wipe out humanity.\",\n",
    "        \"author\": \"Michael Crichton\",\n",
    "        \"year\": 1969,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"The Left Hand of Darkness\",\n",
    "        \"description\": \"A human ambassador is sent to a planet where the inhabitants are genderless and can change gender at will.\",\n",
    "        \"author\": \"Ursula K. Le Guin\",\n",
    "        \"year\": 1969,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"The Three-Body Problem\",\n",
    "        \"description\": \"Humans encounter an alien civilization that lives in a dying system.\",\n",
    "        \"author\": \"Liu Cixin\",\n",
    "        \"year\": 2008,\n",
    "    },\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'code': 128512, 'char': 'ðŸ˜€', 'name': 'GRINNING FACE'}, {'code': 128513, 'char': 'ðŸ˜', 'name': 'GRINNING FACE WITH SMILING EYES'}, {'code': 128514, 'char': 'ðŸ˜‚', 'name': 'FACE WITH TEARS OF JOY'}, {'code': 128515, 'char': 'ðŸ˜ƒ', 'name': 'SMILING FACE WITH OPEN MOUTH'}, {'code': 128516, 'char': 'ðŸ˜„', 'name': 'SMILING FACE WITH OPEN MOUTH AND SMILING EYES'}, {'code': 128517, 'char': 'ðŸ˜…', 'name': 'SMILING FACE WITH OPEN MOUTH AND COLD SWEAT'}, {'code': 128518, 'char': 'ðŸ˜†', 'name': 'SMILING FACE WITH OPEN MOUTH AND TIGHTLY-CLOSED EYES'}, {'code': 128519, 'char': 'ðŸ˜‡', 'name': 'SMILING FACE WITH HALO'}, {'code': 128520, 'char': 'ðŸ˜ˆ', 'name': 'SMILING FACE WITH HORNS'}, {'code': 128521, 'char': 'ðŸ˜‰', 'name': 'WINKING FACE'}]\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "\n",
    "def get_country_flags():\n",
    "    flags = []\n",
    "    base = 0x1F1E6  # Start of regional indicator symbols\n",
    "    for code1 in range(base, base + 26):  # Loop over A to Z\n",
    "        for code2 in range(base, base + 26):  # Loop over A to Z\n",
    "            flag_char = chr(code1) + chr(code2)\n",
    "            flag_name = f\"Flag of {chr(code1 - base + 65)}{chr(code2 - base + 65)}\"\n",
    "            flags.append({'code': ord(flag_char[0]) * 0x10000 + ord(flag_char[1]), 'char': flag_char, 'name': flag_name})\n",
    "    return flags\n",
    "\n",
    "def get_standard_emojis():\n",
    "    emojis = []\n",
    "    # Ranges of emojis (excluding country flags)\n",
    "    emoji_ranges = [\n",
    "        (0x1F600, 0x1F64F),  # Emoticons\n",
    "        (0x1F300, 0x1F5FF),  # Miscellaneous Symbols and Pictographs\n",
    "        (0x1F680, 0x1F6FF),  # Transport and Map Symbols\n",
    "        (0x1F700, 0x1F77F),  # Alchemical Symbols\n",
    "        (0x2600, 0x26FF),    # Miscellaneous Symbols\n",
    "        (0x2700, 0x27BF),    # Dingbats\n",
    "        (0x2B50, 0x2BFF),    # Additional symbols\n",
    "    ]\n",
    "    for start, end in emoji_ranges:\n",
    "        for code in range(start, end + 1):\n",
    "            try:\n",
    "                char = chr(code)\n",
    "                name = unicodedata.name(char)\n",
    "                emojis.append({'code': code, 'char': char, 'name': name})\n",
    "            except ValueError:\n",
    "                continue\n",
    "    return emojis\n",
    "\n",
    "# Combine all emojis and flags into one list\n",
    "emojis = get_standard_emojis() + get_country_flags()\n",
    "\n",
    "# Print the first few for demonstration\n",
    "print(emojis[:10])  # Print only the first 10 for brevity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ðŸ‡©ðŸ‡ª'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def print_flag(country_code):\n",
    "    assert len(country_code) == 2 and country_code.isalpha(), \"Country code must be two alphabetical characters\"\n",
    "    base = 0x1F1E6\n",
    "    flag = chr(base + ord(country_code[0]) - ord('A')) + chr(base + ord(country_code[1]) - ord('A'))\n",
    "    return str(flag) \n",
    "\n",
    "# Example usage:\n",
    "print_flag('US')  # Outputs: ðŸ‡ºðŸ‡¸\n",
    "print_flag('DE')  # Outputs: ðŸ‡©ðŸ‡ª\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'code': 127888, 'char': 'ðŸŽ', 'name': 'Wind chime'}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emojis[765]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_729654/1368592882.py:1: DeprecationWarning: `recreate_collection` method is deprecated and will be removed in the future. Use `collection_exists` to check collection existence and `create_collection` instead.\n",
      "  client.recreate_collection(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.recreate_collection(\n",
    "    collection_name=\"my_books\",\n",
    "    vectors_config=models.VectorParams(\n",
    "        size=encoder.get_sentence_embedding_dimension(),\n",
    "        distance=models.Distance.COSINE,\n",
    "    ),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.upload_points(\n",
    "    collection_name=\"my_books\",\n",
    "    points=[\n",
    "        models.PointStruct(\n",
    "            id=idx, vector=encoder.encode(emoji[\"name\"]).tolist(), payload=emoji\n",
    "        )\n",
    "        for idx, emoji in enumerate(emojis)\n",
    "    ],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ–\n",
      "ðŸ‘™\n",
      "ðŸ\n",
      "ðŸ„\n",
      "ðŸœ\n",
      "ðŸ¬\n",
      "ðŸ‹\n",
      "ðŸ“Ÿ\n",
      "â™\n",
      "â™†\n"
     ]
    }
   ],
   "source": [
    "hits = client.search(\n",
    "    collection_name=\"my_books\",\n",
    "    query_vector=encoder.encode(\"beach\").tolist(),\n",
    "    limit=10,\n",
    ")\n",
    "\n",
    "for hit in hits:\n",
    "    print(hit.payload['char']) #, \"score:\", hit.score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emoscope",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
